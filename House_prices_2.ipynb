{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([train_df,test_df],axis=0,sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500.0\n",
       "1    181500.0\n",
       "2    223500.0\n",
       "3    140000.0\n",
       "4    250000.0\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['Alley'],axis=1,inplace=True)\n",
    "final_df.drop(['FireplaceQu'],axis=1,inplace=True)\n",
    "final_df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(final_df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['LotFrontage']=final_df['LotFrontage'].fillna(final_df['LotFrontage'].mean())\n",
    "final_df['MasVnrArea']=final_df['MasVnrArea'].fillna(final_df['MasVnrArea'].mean())\n",
    "final_df['BsmtFinSF1']=final_df['BsmtFinSF1'].fillna(final_df['BsmtFinSF1'].mean())\n",
    "final_df['BsmtFinSF2']=final_df['BsmtFinSF2'].fillna(final_df['BsmtFinSF2'].mean())\n",
    "final_df['BsmtUnfSF']=final_df['BsmtUnfSF'].fillna(final_df['BsmtUnfSF'].mean())\n",
    "final_df['TotalBsmtSF']=final_df['TotalBsmtSF'].fillna(final_df['TotalBsmtSF'].mean())\n",
    "final_df['BsmtFullBath']=final_df['BsmtFullBath'].fillna(final_df['BsmtFullBath'].mean())\n",
    "final_df['BsmtHalfBath']=final_df['BsmtHalfBath'].fillna(final_df['BsmtHalfBath'].mean())\n",
    "final_df['GarageYrBlt']=final_df['GarageYrBlt'].fillna(final_df['GarageYrBlt'].mean())\n",
    "final_df['GarageCars']=final_df['GarageCars'].fillna(final_df['GarageCars'].mean())\n",
    "final_df['GarageArea']=final_df['GarageArea'].fillna(final_df['GarageArea'].mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['MSZoning']=final_df['MSZoning'].fillna(final_df['MSZoning'].mode()[0])\n",
    "final_df['Utilities']=final_df['Utilities'].fillna(final_df['Utilities'].mode()[0])\n",
    "final_df['Exterior1st']=final_df['Exterior1st'].fillna(final_df['Exterior1st'].mode()[0])\n",
    "final_df['Exterior2nd']=final_df['Exterior2nd'].fillna(final_df['Exterior2nd'].mode()[0])\n",
    "final_df['MasVnrType']=final_df['MasVnrType'].fillna(final_df['MasVnrType'].mode()[0])\n",
    "final_df['BsmtQual']=final_df['BsmtQual'].fillna(final_df['BsmtQual'].mode()[0])\n",
    "final_df['BsmtCond']=final_df['BsmtCond'].fillna(final_df['BsmtCond'].mode()[0])\n",
    "final_df['BsmtExposure']=final_df['BsmtExposure'].fillna(final_df['BsmtExposure'].mode()[0])\n",
    "final_df['BsmtFinType1']=final_df['BsmtFinType1'].fillna(final_df['BsmtFinType1'].mode()[0])\n",
    "final_df['BsmtFinType2']=final_df['BsmtFinType2'].fillna(final_df['BsmtFinType2'].mode()[0])\n",
    "final_df['Electrical']=final_df['Electrical'].fillna(final_df['Electrical'].mode()[0])\n",
    "final_df['KitchenQual']=final_df['KitchenQual'].fillna(final_df['KitchenQual'].mode()[0])\n",
    "final_df['Functional']=final_df['Functional'].fillna(final_df['Functional'].mode()[0])\n",
    "final_df['GarageType']=final_df['GarageType'].fillna(final_df['GarageType'].mode()[0])\n",
    "final_df['GarageFinish']=final_df['GarageFinish'].fillna(final_df['GarageFinish'].mode()[0])\n",
    "final_df['GarageQual']=final_df['GarageQual'].fillna(final_df['GarageQual'].mode()[0])\n",
    "final_df['GarageCond']=final_df['GarageCond'].fillna(final_df['GarageCond'].mode()[0])\n",
    "final_df['SaleType']=final_df['SaleType'].fillna(final_df['SaleType'].mode()[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 76)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 75)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(final_df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e608569470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(final_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSZoning',\n",
       " 'Street',\n",
       " 'LotShape',\n",
       " 'LandContour',\n",
       " 'Utilities',\n",
       " 'LotConfig',\n",
       " 'LandSlope',\n",
       " 'Neighborhood',\n",
       " 'Condition1',\n",
       " 'Condition2',\n",
       " 'BldgType',\n",
       " 'HouseStyle',\n",
       " 'RoofStyle',\n",
       " 'RoofMatl',\n",
       " 'Exterior1st',\n",
       " 'Exterior2nd',\n",
       " 'MasVnrType',\n",
       " 'ExterQual',\n",
       " 'ExterCond',\n",
       " 'Foundation',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'Heating',\n",
       " 'HeatingQC',\n",
       " 'CentralAir',\n",
       " 'Electrical',\n",
       " 'KitchenQual',\n",
       " 'Functional',\n",
       " 'GarageType',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'PavedDrive',\n",
       " 'SaleType',\n",
       " 'SaleCondition']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_var = [var for var in final_df.columns if final_df[var].dtypes == 'O']\n",
    "cat_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 75)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(cat_features):\n",
    "    df_fin=final_df\n",
    "    i=0\n",
    "    for fields in cat_features:\n",
    "        \n",
    "        print(fields)\n",
    "        \n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        \n",
    "        if i==0:\n",
    "            df_fin=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_fin=pd.concat([df_fin,df1],axis=1)\n",
    "        \n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_fin=pd.concat([final_df,df_fin],axis=1)\n",
    "        \n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition1\n",
      "Condition2\n",
      "BldgType\n",
      "HouseStyle\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "ExterQual\n",
      "ExterCond\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n",
      "SaleType\n",
      "SaleCondition\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(cat_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 233)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 177)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split ,Feature Scaling, Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1460,:]\n",
    "df_Test=final_df.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 177)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.head()\n",
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 176)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 177)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "5       143000.0\n",
       "6       307000.0\n",
       "7       200000.0\n",
       "8       129900.0\n",
       "9       118000.0\n",
       "10      129500.0\n",
       "11      345000.0\n",
       "12      144000.0\n",
       "13      279500.0\n",
       "14      157000.0\n",
       "15      132000.0\n",
       "16      149000.0\n",
       "17       90000.0\n",
       "18      159000.0\n",
       "19      139000.0\n",
       "20      325300.0\n",
       "21      139400.0\n",
       "22      230000.0\n",
       "23      129900.0\n",
       "24      154000.0\n",
       "25      256300.0\n",
       "26      134800.0\n",
       "27      306000.0\n",
       "28      207500.0\n",
       "29       68500.0\n",
       "          ...   \n",
       "1430    192140.0\n",
       "1431    143750.0\n",
       "1432     64500.0\n",
       "1433    186500.0\n",
       "1434    160000.0\n",
       "1435    174000.0\n",
       "1436    120500.0\n",
       "1437    394617.0\n",
       "1438    149700.0\n",
       "1439    197000.0\n",
       "1440    191000.0\n",
       "1441    149300.0\n",
       "1442    310000.0\n",
       "1443    121000.0\n",
       "1444    179600.0\n",
       "1445    129000.0\n",
       "1446    157900.0\n",
       "1447    240000.0\n",
       "1448    112000.0\n",
       "1449     92000.0\n",
       "1450    136000.0\n",
       "1451    287090.0\n",
       "1452    145000.0\n",
       "1453     84500.0\n",
       "1454    185000.0\n",
       "1455    175000.0\n",
       "1456    210000.0\n",
       "1457    266500.0\n",
       "1458    142125.0\n",
       "1459    147500.0\n",
       "Name: SalePrice, Length: 1460, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "df_Test = sc.transform(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 176)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 176)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components = None)\n",
    "X_train = pca1.fit_transform(X_train)\n",
    "df_Test = pca1.transform(df_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca1.explained_variance_ratio_\n",
    "print(explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(explained_variance,bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_feat = explained_variance[explained_variance > 0.0035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_feat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will select 103 most imp features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 105)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "df_Test = pca.transform(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "classifier=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameter Tuning\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "base_score=[0.25,0.5,0.75,1]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "random_cv = RandomizedSearchCV(estimator=classifier,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs =-1,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=3, missing=None, n_estimators=1100,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on data set\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 176)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 176)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=176, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=40, kernel_initializer=\"he_uniform\")`\n",
      "  \n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\sethi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - 0s 268us/step - loss: 94.5952 - val_loss: 72.2448\n",
      "Epoch 2/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 60.3297 - val_loss: 51.7485\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 44.6374 - val_loss: 39.8505\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 34.8066 - val_loss: 31.8717\n",
      "Epoch 5/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 27.9584 - val_loss: 26.0823\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22.8771 - val_loss: 21.6640\n",
      "Epoch 7/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 18.9485 - val_loss: 18.1798\n",
      "Epoch 8/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 15.8228 - val_loss: 15.3581\n",
      "Epoch 9/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 13.2783 - val_loss: 13.0433\n",
      "Epoch 10/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 11.1765 - val_loss: 11.1114\n",
      "Epoch 11/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 9.4173 - val_loss: 9.4819\n",
      "Epoch 12/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 7.9322 - val_loss: 8.0988\n",
      "Epoch 13/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 6.6730 - val_loss: 6.9155\n",
      "Epoch 14/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 5.6005 - val_loss: 5.9052\n",
      "Epoch 15/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 4.6855 - val_loss: 5.0406\n",
      "Epoch 16/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 3.9032 - val_loss: 4.2988\n",
      "Epoch 17/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 3.2349 - val_loss: 3.6630\n",
      "Epoch 18/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 2.6655 - val_loss: 3.1217\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 2.1822 - val_loss: 2.6581\n",
      "Epoch 20/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 1.7741 - val_loss: 2.2664\n",
      "Epoch 21/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1.4322 - val_loss: 1.9359\n",
      "Epoch 22/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 1.1475 - val_loss: 1.6595\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 0.9120 - val_loss: 1.4306\n",
      "Epoch 24/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.7199 - val_loss: 1.2430\n",
      "Epoch 25/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 0.5647 - val_loss: 1.0911\n",
      "Epoch 26/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 0.4410 - val_loss: 0.9681\n",
      "Epoch 27/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.3438 - val_loss: 0.8705\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 0.2685 - val_loss: 0.7939\n",
      "Epoch 29/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 0.2111 - val_loss: 0.7347\n",
      "Epoch 30/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.1680 - val_loss: 0.6881\n",
      "Epoch 31/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1361 - val_loss: 0.6525\n",
      "Epoch 32/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 0.1126 - val_loss: 0.6252\n",
      "Epoch 33/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0951 - val_loss: 0.6045\n",
      "Epoch 34/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0821 - val_loss: 0.5883\n",
      "Epoch 35/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0721 - val_loss: 0.5757\n",
      "Epoch 36/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 0.0642 - val_loss: 0.5654\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0579 - val_loss: 0.5562\n",
      "Epoch 38/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0528 - val_loss: 0.5485\n",
      "Epoch 39/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0486 - val_loss: 0.5418\n",
      "Epoch 40/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0450 - val_loss: 0.5362\n",
      "Epoch 41/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 0.0419 - val_loss: 0.5317\n",
      "Epoch 42/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0393 - val_loss: 0.5272\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 0.0369 - val_loss: 0.5231\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0348 - val_loss: 0.5195\n",
      "Epoch 45/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 0.0328 - val_loss: 0.5166\n",
      "Epoch 46/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 0.0310 - val_loss: 0.5140\n",
      "Epoch 47/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0294 - val_loss: 0.5111\n",
      "Epoch 48/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 0.0279 - val_loss: 0.5092\n",
      "Epoch 49/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 0.0265 - val_loss: 0.5072\n",
      "Epoch 50/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0251 - val_loss: 0.5047\n",
      "Epoch 51/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 0.0239 - val_loss: 0.5036\n",
      "Epoch 52/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 0.0228 - val_loss: 0.5022\n",
      "Epoch 53/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0219 - val_loss: 0.5017\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 0.0208 - val_loss: 0.5010\n",
      "Epoch 55/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0200 - val_loss: 0.5011\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 0.0191 - val_loss: 0.5005\n",
      "Epoch 57/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 0.0184 - val_loss: 0.4997\n",
      "Epoch 58/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0177 - val_loss: 0.4996\n",
      "Epoch 59/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 0.0170 - val_loss: 0.5006\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - 0s 203us/step - loss: 0.0164 - val_loss: 0.5005\n",
      "Epoch 61/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0158 - val_loss: 0.5002\n",
      "Epoch 62/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0152 - val_loss: 0.5000\n",
      "Epoch 63/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0147 - val_loss: 0.4999\n",
      "Epoch 64/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.0142 - val_loss: 0.5004\n",
      "Epoch 65/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0138 - val_loss: 0.5014\n",
      "Epoch 66/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0134 - val_loss: 0.5002\n",
      "Epoch 67/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.0129 - val_loss: 0.4996\n",
      "Epoch 68/1000\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 0.0125 - val_loss: 0.4980\n",
      "Epoch 69/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0122 - val_loss: 0.4975\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0118 - val_loss: 0.4964\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0115 - val_loss: 0.4964\n",
      "Epoch 72/1000\n",
      "1168/1168 [==============================] - 0s 223us/step - loss: 0.0111 - val_loss: 0.4952\n",
      "Epoch 73/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0109 - val_loss: 0.4942\n",
      "Epoch 74/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0106 - val_loss: 0.4943\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0103 - val_loss: 0.4930\n",
      "Epoch 76/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0100 - val_loss: 0.4932\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0098 - val_loss: 0.4925\n",
      "Epoch 78/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0095 - val_loss: 0.4899\n",
      "Epoch 79/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0093 - val_loss: 0.4883\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0091 - val_loss: 0.4869\n",
      "Epoch 81/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0089 - val_loss: 0.4849\n",
      "Epoch 82/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0087 - val_loss: 0.4840\n",
      "Epoch 83/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0085 - val_loss: 0.4833\n",
      "Epoch 84/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0084 - val_loss: 0.4788\n",
      "Epoch 85/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0082 - val_loss: 0.4789\n",
      "Epoch 86/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0080 - val_loss: 0.4768\n",
      "Epoch 87/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0079 - val_loss: 0.4747\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0078 - val_loss: 0.4726\n",
      "Epoch 89/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0076 - val_loss: 0.4713\n",
      "Epoch 90/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0074 - val_loss: 0.4683\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0073 - val_loss: 0.4638\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0072 - val_loss: 0.4604\n",
      "Epoch 93/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0070 - val_loss: 0.4609\n",
      "Epoch 94/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0069 - val_loss: 0.4574\n",
      "Epoch 95/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0068 - val_loss: 0.4537\n",
      "Epoch 96/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0067 - val_loss: 0.4502\n",
      "Epoch 97/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.0066 - val_loss: 0.4474\n",
      "Epoch 98/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0065 - val_loss: 0.4441\n",
      "Epoch 99/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0064 - val_loss: 0.4420\n",
      "Epoch 100/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0063 - val_loss: 0.4382\n",
      "Epoch 101/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0062 - val_loss: 0.4343\n",
      "Epoch 102/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0061 - val_loss: 0.4283\n",
      "Epoch 103/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0060 - val_loss: 0.4261\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0059 - val_loss: 0.4208\n",
      "Epoch 105/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0058 - val_loss: 0.4170\n",
      "Epoch 106/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0058 - val_loss: 0.4133\n",
      "Epoch 107/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0056 - val_loss: 0.4071\n",
      "Epoch 108/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0056 - val_loss: 0.4030\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0055 - val_loss: 0.3978\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0054 - val_loss: 0.3933\n",
      "Epoch 111/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0053 - val_loss: 0.3872\n",
      "Epoch 112/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0053 - val_loss: 0.3826\n",
      "Epoch 113/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0052 - val_loss: 0.3767\n",
      "Epoch 114/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0051 - val_loss: 0.3706\n",
      "Epoch 115/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0051 - val_loss: 0.3670\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0050 - val_loss: 0.3553\n",
      "Epoch 117/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0049 - val_loss: 0.3434\n",
      "Epoch 118/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0049 - val_loss: 0.3326\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0048 - val_loss: 0.3192\n",
      "Epoch 120/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0048 - val_loss: 0.3116\n",
      "Epoch 121/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0047 - val_loss: 0.3086\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0047 - val_loss: 0.2973\n",
      "Epoch 123/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0046 - val_loss: 0.2901\n",
      "Epoch 124/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0046 - val_loss: 0.2850\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0045 - val_loss: 0.2743\n",
      "Epoch 126/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0044 - val_loss: 0.2672\n",
      "Epoch 127/1000\n",
      "1168/1168 [==============================] - 0s 229us/step - loss: 0.0044 - val_loss: 0.2606\n",
      "Epoch 128/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 0.0043 - val_loss: 0.2511\n",
      "Epoch 129/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0043 - val_loss: 0.2495\n",
      "Epoch 130/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0042 - val_loss: 0.2438\n",
      "Epoch 131/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0042 - val_loss: 0.2368\n",
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0041 - val_loss: 0.2336\n",
      "Epoch 133/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0041 - val_loss: 0.2266\n",
      "Epoch 134/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0040 - val_loss: 0.2215\n",
      "Epoch 135/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.2134\n",
      "Epoch 136/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0039 - val_loss: 0.2080\n",
      "Epoch 137/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0039 - val_loss: 0.2034\n",
      "Epoch 138/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0039 - val_loss: 0.1978\n",
      "Epoch 139/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0038 - val_loss: 0.1963\n",
      "Epoch 140/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.1906\n",
      "Epoch 141/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.1849\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0037 - val_loss: 0.1812\n",
      "Epoch 143/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0037 - val_loss: 0.1784\n",
      "Epoch 144/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.1748\n",
      "Epoch 145/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0036 - val_loss: 0.1710\n",
      "Epoch 146/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.1681\n",
      "Epoch 147/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.1644\n",
      "Epoch 148/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0035 - val_loss: 0.1616\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.1591\n",
      "Epoch 150/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.1557\n",
      "Epoch 151/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0034 - val_loss: 0.1520\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0033 - val_loss: 0.1497\n",
      "Epoch 153/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0033 - val_loss: 0.1462\n",
      "Epoch 154/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 0.0033 - val_loss: 0.1424\n",
      "Epoch 155/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.1407\n",
      "Epoch 156/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.1378\n",
      "Epoch 157/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0032 - val_loss: 0.1341\n",
      "Epoch 158/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.1322\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0031 - val_loss: 0.1281\n",
      "Epoch 160/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0031 - val_loss: 0.1247\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0031 - val_loss: 0.1214\n",
      "Epoch 162/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0030 - val_loss: 0.1199\n",
      "Epoch 163/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.1174\n",
      "Epoch 164/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.1132\n",
      "Epoch 165/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.1123\n",
      "Epoch 166/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.1101\n",
      "Epoch 167/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.1080\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.1051\n",
      "Epoch 169/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.1029\n",
      "Epoch 170/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.1010\n",
      "Epoch 171/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 0.0028 - val_loss: 0.0984\n",
      "Epoch 172/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.0028 - val_loss: 0.0959\n",
      "Epoch 173/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 0.0027 - val_loss: 0.0939\n",
      "Epoch 174/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 0.0027 - val_loss: 0.0933\n",
      "Epoch 175/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0027 - val_loss: 0.0917\n",
      "Epoch 176/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 0.0027 - val_loss: 0.0899\n",
      "Epoch 177/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 0.0026 - val_loss: 0.0874\n",
      "Epoch 178/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 0.0026 - val_loss: 0.0851\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0026 - val_loss: 0.0841\n",
      "Epoch 180/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.0025 - val_loss: 0.0827\n",
      "Epoch 181/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0025 - val_loss: 0.0795\n",
      "Epoch 182/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0025 - val_loss: 0.0787\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0025 - val_loss: 0.0768\n",
      "Epoch 184/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0024 - val_loss: 0.0752\n",
      "Epoch 185/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0024 - val_loss: 0.0745\n",
      "Epoch 186/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0024 - val_loss: 0.0715\n",
      "Epoch 187/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0024 - val_loss: 0.0713\n",
      "Epoch 188/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 0.0024 - val_loss: 0.0700\n",
      "Epoch 189/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0023 - val_loss: 0.0687\n",
      "Epoch 190/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0023 - val_loss: 0.0671\n",
      "Epoch 191/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0023 - val_loss: 0.0659\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 0.0023 - val_loss: 0.0639\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0022 - val_loss: 0.0632\n",
      "Epoch 194/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0023 - val_loss: 0.0619\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0022 - val_loss: 0.0616\n",
      "Epoch 196/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 0.0022 - val_loss: 0.0593\n",
      "Epoch 197/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0022 - val_loss: 0.0590\n",
      "Epoch 198/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0021 - val_loss: 0.0575\n",
      "Epoch 199/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 0.0021 - val_loss: 0.0565\n",
      "Epoch 200/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 0.0021 - val_loss: 0.0559\n",
      "Epoch 201/1000\n",
      "1168/1168 [==============================] - 0s 197us/step - loss: 0.0021 - val_loss: 0.0550\n",
      "Epoch 202/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 0.0021 - val_loss: 0.0546\n",
      "Epoch 203/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0021 - val_loss: 0.0532\n",
      "Epoch 204/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0020 - val_loss: 0.0521\n",
      "Epoch 205/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0020 - val_loss: 0.0513\n",
      "Epoch 206/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0020 - val_loss: 0.0506\n",
      "Epoch 207/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0020 - val_loss: 0.0496\n",
      "Epoch 208/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0019 - val_loss: 0.0490\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0019 - val_loss: 0.0481\n",
      "Epoch 210/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0019 - val_loss: 0.0471\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0019 - val_loss: 0.0460\n",
      "Epoch 212/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 0.0019 - val_loss: 0.0461\n",
      "Epoch 213/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0019 - val_loss: 0.0450\n",
      "Epoch 214/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0019 - val_loss: 0.0446\n",
      "Epoch 215/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 0.0018 - val_loss: 0.0435\n",
      "Epoch 216/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0018 - val_loss: 0.0428\n",
      "Epoch 217/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0018 - val_loss: 0.0422\n",
      "Epoch 218/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0018 - val_loss: 0.0416\n",
      "Epoch 219/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0018 - val_loss: 0.0407\n",
      "Epoch 220/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0018 - val_loss: 0.0402\n",
      "Epoch 221/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0018 - val_loss: 0.0398\n",
      "Epoch 222/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0017 - val_loss: 0.0391\n",
      "Epoch 223/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0017 - val_loss: 0.0391\n",
      "Epoch 224/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0017 - val_loss: 0.0384\n",
      "Epoch 225/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0017 - val_loss: 0.0377\n",
      "Epoch 226/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0017 - val_loss: 0.0376\n",
      "Epoch 227/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0016 - val_loss: 0.0375\n",
      "Epoch 228/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.0016 - val_loss: 0.0371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 0.0016 - val_loss: 0.0363\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0016 - val_loss: 0.0361\n",
      "Epoch 231/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0016 - val_loss: 0.0356\n",
      "Epoch 232/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0016 - val_loss: 0.0355\n",
      "Epoch 233/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0016 - val_loss: 0.0351\n",
      "Epoch 234/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0016 - val_loss: 0.0350\n",
      "Epoch 235/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0016 - val_loss: 0.0343\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0015 - val_loss: 0.0342\n",
      "Epoch 237/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0015 - val_loss: 0.0338\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0015 - val_loss: 0.0337\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0015 - val_loss: 0.0332\n",
      "Epoch 240/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0015 - val_loss: 0.0331\n",
      "Epoch 241/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0015 - val_loss: 0.0328\n",
      "Epoch 242/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0015 - val_loss: 0.0326\n",
      "Epoch 243/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0014 - val_loss: 0.0325\n",
      "Epoch 244/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 0.0014 - val_loss: 0.0324\n",
      "Epoch 245/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 0.0014 - val_loss: 0.0321\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0014 - val_loss: 0.0318\n",
      "Epoch 247/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0014 - val_loss: 0.0315\n",
      "Epoch 248/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0014 - val_loss: 0.0316\n",
      "Epoch 249/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 0.0014 - val_loss: 0.0315\n",
      "Epoch 250/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0013 - val_loss: 0.0311\n",
      "Epoch 251/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0013 - val_loss: 0.0311\n",
      "Epoch 252/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0013 - val_loss: 0.0310\n",
      "Epoch 253/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0013 - val_loss: 0.0311\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0013 - val_loss: 0.0307\n",
      "Epoch 255/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 0.0013 - val_loss: 0.0307\n",
      "Epoch 256/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 0.0013 - val_loss: 0.0308\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 0.0013 - val_loss: 0.0306\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0013 - val_loss: 0.0305\n",
      "Epoch 259/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0013 - val_loss: 0.0305\n",
      "Epoch 260/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 0.0012 - val_loss: 0.0302\n",
      "Epoch 261/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 0.0012 - val_loss: 0.0303\n",
      "Epoch 262/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0012 - val_loss: 0.0304\n",
      "Epoch 263/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0012 - val_loss: 0.0300\n",
      "Epoch 264/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0012 - val_loss: 0.0300\n",
      "Epoch 265/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0012 - val_loss: 0.0300\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0012 - val_loss: 0.0302\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 0.0012 - val_loss: 0.0300\n",
      "Epoch 268/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0012 - val_loss: 0.0301\n",
      "Epoch 269/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 0.0012 - val_loss: 0.0299\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 0.0011 - val_loss: 0.0299\n",
      "Epoch 271/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 0.0011 - val_loss: 0.0299\n",
      "Epoch 272/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0011 - val_loss: 0.0301\n",
      "Epoch 273/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0011 - val_loss: 0.0302\n",
      "Epoch 274/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0011 - val_loss: 0.0299\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 0.0011 - val_loss: 0.0301\n",
      "Epoch 276/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 0.0011 - val_loss: 0.0299\n",
      "Epoch 277/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 0.0011 - val_loss: 0.0301\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 0.0011 - val_loss: 0.0302\n",
      "Epoch 279/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 0.0011 - val_loss: 0.0302\n",
      "Epoch 280/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 0.0011 - val_loss: 0.0303\n",
      "Epoch 281/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 0.0010 - val_loss: 0.0303\n",
      "Epoch 282/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 0.0010 - val_loss: 0.0304\n",
      "Epoch 283/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 0.0010 - val_loss: 0.0303\n",
      "Epoch 284/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 0.0010 - val_loss: 0.0305\n",
      "Epoch 285/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 0.0010 - val_loss: 0.0306\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.9222e-04 - val_loss: 0.0306\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 9.9653e-04 - val_loss: 0.0308\n",
      "Epoch 288/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 9.7680e-04 - val_loss: 0.0310\n",
      "Epoch 289/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.7469e-04 - val_loss: 0.0308\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9.7064e-04 - val_loss: 0.0309\n",
      "Epoch 291/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.6798e-04 - val_loss: 0.0310\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.4588e-04 - val_loss: 0.0313\n",
      "Epoch 293/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.4677e-04 - val_loss: 0.0312\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.4675e-04 - val_loss: 0.0312\n",
      "Epoch 295/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 9.4899e-04 - val_loss: 0.0313\n",
      "Epoch 296/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 9.3850e-04 - val_loss: 0.0314\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 9.0707e-04 - val_loss: 0.0315\n",
      "Epoch 298/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.0039e-04 - val_loss: 0.0315\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 8.9507e-04 - val_loss: 0.0317\n",
      "Epoch 300/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 9.0272e-04 - val_loss: 0.0320\n",
      "Epoch 301/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 9.0697e-04 - val_loss: 0.0319\n",
      "Epoch 302/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 8.7835e-04 - val_loss: 0.0322\n",
      "Epoch 303/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 8.6593e-04 - val_loss: 0.0322\n",
      "Epoch 304/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 8.7176e-04 - val_loss: 0.0324\n",
      "Epoch 305/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8.6378e-04 - val_loss: 0.0325\n",
      "Epoch 306/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 8.5934e-04 - val_loss: 0.0326\n",
      "Epoch 307/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 8.5417e-04 - val_loss: 0.0327\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 8.4827e-04 - val_loss: 0.0331\n",
      "Epoch 309/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 8.3323e-04 - val_loss: 0.0330\n",
      "Epoch 310/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 8.3520e-04 - val_loss: 0.0333\n",
      "Epoch 311/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8.2378e-04 - val_loss: 0.0334\n",
      "Epoch 312/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 8.1985e-04 - val_loss: 0.0334\n",
      "Epoch 313/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 8.2454e-04 - val_loss: 0.0337\n",
      "Epoch 314/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.9918e-04 - val_loss: 0.0338\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 8.1822e-04 - val_loss: 0.0340\n",
      "Epoch 316/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.8341e-04 - val_loss: 0.0341\n",
      "Epoch 317/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 8.0023e-04 - val_loss: 0.0342\n",
      "Epoch 318/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 7.7385e-04 - val_loss: 0.0344\n",
      "Epoch 319/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 7.7684e-04 - val_loss: 0.0347\n",
      "Epoch 320/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.9067e-04 - val_loss: 0.0346\n",
      "Epoch 321/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 7.6374e-04 - val_loss: 0.0349\n",
      "Epoch 322/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.5906e-04 - val_loss: 0.0352\n",
      "Epoch 323/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 7.5027e-04 - val_loss: 0.0354\n",
      "Epoch 324/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 7.4144e-04 - val_loss: 0.0354\n",
      "Epoch 325/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 7.3674e-04 - val_loss: 0.0356\n",
      "Epoch 326/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 7.2705e-04 - val_loss: 0.0357\n",
      "Epoch 327/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.1910e-04 - val_loss: 0.0359\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.1773e-04 - val_loss: 0.0360\n",
      "Epoch 329/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 7.1642e-04 - val_loss: 0.0363\n",
      "Epoch 330/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.2436e-04 - val_loss: 0.0366\n",
      "Epoch 331/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 7.2066e-04 - val_loss: 0.0366\n",
      "Epoch 332/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 7.1291e-04 - val_loss: 0.0367\n",
      "Epoch 333/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 6.9142e-04 - val_loss: 0.0369\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 6.9146e-04 - val_loss: 0.0372\n",
      "Epoch 335/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.8591e-04 - val_loss: 0.0374\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.9135e-04 - val_loss: 0.0375\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6.6960e-04 - val_loss: 0.0379\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 6.7847e-04 - val_loss: 0.0379\n",
      "Epoch 339/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 6.6936e-04 - val_loss: 0.0383\n",
      "Epoch 340/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 6.6818e-04 - val_loss: 0.0384\n",
      "Epoch 341/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 6.7088e-04 - val_loss: 0.0386\n",
      "Epoch 342/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 6.5061e-04 - val_loss: 0.0385\n",
      "Epoch 343/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6.4048e-04 - val_loss: 0.0391\n",
      "Epoch 344/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 6.5491e-04 - val_loss: 0.0392\n",
      "Epoch 345/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.4011e-04 - val_loss: 0.0393\n",
      "Epoch 346/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6.3310e-04 - val_loss: 0.0395\n",
      "Epoch 347/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 6.3078e-04 - val_loss: 0.0397\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 6.1715e-04 - val_loss: 0.0397\n",
      "Epoch 349/1000\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 6.1920e-04 - val_loss: 0.0399\n",
      "Epoch 350/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 6.1646e-04 - val_loss: 0.0401\n",
      "Epoch 351/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6.1520e-04 - val_loss: 0.0404\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6.0599e-04 - val_loss: 0.0405\n",
      "Epoch 353/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 6.0187e-04 - val_loss: 0.0408\n",
      "Epoch 354/1000\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 5.9744e-04 - val_loss: 0.0408\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - 0s 206us/step - loss: 5.9966e-04 - val_loss: 0.0410\n",
      "Epoch 356/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 5.9008e-04 - val_loss: 0.0413\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 5.8730e-04 - val_loss: 0.0414\n",
      "Epoch 358/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5.7876e-04 - val_loss: 0.0417\n",
      "Epoch 359/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 5.8044e-04 - val_loss: 0.0421\n",
      "Epoch 360/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 5.8420e-04 - val_loss: 0.0423\n",
      "Epoch 361/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 5.6894e-04 - val_loss: 0.0423\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 5.7558e-04 - val_loss: 0.0425\n",
      "Epoch 363/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 5.6434e-04 - val_loss: 0.0429\n",
      "Epoch 364/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 5.9303e-0 - 0s 175us/step - loss: 5.5572e-04 - val_loss: 0.0428\n",
      "Epoch 365/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 5.5454e-04 - val_loss: 0.0432\n",
      "Epoch 366/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 5.5287e-04 - val_loss: 0.0434\n",
      "Epoch 367/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 5.5935e-04 - val_loss: 0.0436\n",
      "Epoch 368/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 5.4470e-04 - val_loss: 0.0437\n",
      "Epoch 369/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 5.4056e-04 - val_loss: 0.0441\n",
      "Epoch 370/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 5.4405e-04 - val_loss: 0.0440\n",
      "Epoch 371/1000\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 5.3283e-04 - val_loss: 0.0442\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - 0s 215us/step - loss: 5.3709e-04 - val_loss: 0.0444\n",
      "Epoch 373/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 5.3532e-04 - val_loss: 0.0448\n",
      "Epoch 374/1000\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 5.4232e-04 - val_loss: 0.0450\n",
      "Epoch 375/1000\n",
      "1168/1168 [==============================] - 0s 200us/step - loss: 5.2532e-04 - val_loss: 0.0452\n",
      "Epoch 376/1000\n",
      "1168/1168 [==============================] - 0s 222us/step - loss: 5.1743e-04 - val_loss: 0.0454\n",
      "Epoch 377/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - ETA: 0s - loss: 5.2917e-0 - 0s 188us/step - loss: 5.0650e-04 - val_loss: 0.0453\n",
      "Epoch 378/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 5.1279e-04 - val_loss: 0.0457\n",
      "Epoch 379/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 5.0731e-04 - val_loss: 0.0459\n",
      "Epoch 380/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 5.0524e-04 - val_loss: 0.0463\n",
      "Epoch 381/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 5.0258e-04 - val_loss: 0.0463\n",
      "Epoch 382/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 4.9266e-04 - val_loss: 0.0464\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 5.0158e-04 - val_loss: 0.0467\n",
      "Epoch 384/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.8588e-04 - val_loss: 0.0468\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.9872e-04 - val_loss: 0.0469\n",
      "Epoch 386/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 4.9064e-04 - val_loss: 0.0472\n",
      "Epoch 387/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.8825e-04 - val_loss: 0.0474\n",
      "Epoch 388/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 4.8512e-04 - val_loss: 0.0476\n",
      "Epoch 389/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 4.7196e-04 - val_loss: 0.0478\n",
      "Epoch 390/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 4.7052e-04 - val_loss: 0.0479\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 4.7374e-04 - val_loss: 0.0482\n",
      "Epoch 392/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 4.6514e-04 - val_loss: 0.0485\n",
      "Epoch 393/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.5997e-04 - val_loss: 0.0486\n",
      "Epoch 394/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.6520e-04 - val_loss: 0.0489\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 4.5827e-04 - val_loss: 0.0487\n",
      "Epoch 396/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4.5386e-04 - val_loss: 0.0494\n",
      "Epoch 397/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 4.4396e-04 - val_loss: 0.0494\n",
      "Epoch 398/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4.5294e-04 - val_loss: 0.0500\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 4.4476e-04 - val_loss: 0.0503\n",
      "Epoch 400/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 4.5107e-04 - val_loss: 0.0502\n",
      "Epoch 401/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 4.4583e-04 - val_loss: 0.0506\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 4.2958e-04 - val_loss: 0.0505\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 4.3243e-04 - val_loss: 0.0510\n",
      "Epoch 404/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 4.3458e-04 - val_loss: 0.0511\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.1267e-04 - val_loss: 0.0515\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 4.4224e-04 - val_loss: 0.0516\n",
      "Epoch 407/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 4.1983e-04 - val_loss: 0.0517\n",
      "Epoch 408/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 4.2297e-04 - val_loss: 0.0520\n",
      "Epoch 409/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 4.1133e-04 - val_loss: 0.0521\n",
      "Epoch 410/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 4.3862e-04 - val_loss: 0.0525\n",
      "Epoch 411/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 4.1075e-04 - val_loss: 0.0526\n",
      "Epoch 412/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 4.1092e-04 - val_loss: 0.0529\n",
      "Epoch 413/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 4.2796e-04 - val_loss: 0.0529\n",
      "Epoch 414/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 4.1380e-04 - val_loss: 0.0532\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 3.9736e-04 - val_loss: 0.0534\n",
      "Epoch 416/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 4.0403e-04 - val_loss: 0.0537\n",
      "Epoch 417/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 3.9415e-04 - val_loss: 0.0537\n",
      "Epoch 418/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 3.9424e-04 - val_loss: 0.0540\n",
      "Epoch 419/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 3.8727e-04 - val_loss: 0.0541\n",
      "Epoch 420/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 3.9377e-04 - val_loss: 0.0544\n",
      "Epoch 421/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 4.1510e-04 - val_loss: 0.0544\n",
      "Epoch 422/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 3.9898e-04 - val_loss: 0.0547\n",
      "Epoch 423/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 3.9030e-04 - val_loss: 0.0550\n",
      "Epoch 424/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 3.7813e-04 - val_loss: 0.0552\n",
      "Epoch 425/1000\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 3.8871e-04 - val_loss: 0.0553\n",
      "Epoch 426/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 3.8127e-04 - val_loss: 0.0558\n",
      "Epoch 427/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 3.8704e-04 - val_loss: 0.0559\n",
      "Epoch 428/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 3.7904e-04 - val_loss: 0.0561\n",
      "Epoch 429/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 3.8691e-04 - val_loss: 0.0563\n",
      "Epoch 430/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 3.7073e-04 - val_loss: 0.0564\n",
      "Epoch 431/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 3.7033e-04 - val_loss: 0.0566\n",
      "Epoch 432/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 3.7881e-04 - val_loss: 0.0568\n",
      "Epoch 433/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 3.6141e-04 - val_loss: 0.0571\n",
      "Epoch 434/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.8356e-04 - val_loss: 0.0572\n",
      "Epoch 435/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 3.5226e-04 - val_loss: 0.0573\n",
      "Epoch 436/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 3.5400e-04 - val_loss: 0.0577\n",
      "Epoch 437/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 3.5993e-04 - val_loss: 0.0577\n",
      "Epoch 438/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 3.4109e-04 - val_loss: 0.0579\n",
      "Epoch 439/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 3.3950e-04 - val_loss: 0.0582\n",
      "Epoch 440/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 3.5460e-04 - val_loss: 0.0585\n",
      "Epoch 441/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.6544e-04 - val_loss: 0.0587\n",
      "Epoch 442/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 3.5379e-04 - val_loss: 0.0587\n",
      "Epoch 443/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 3.6009e-04 - val_loss: 0.0590\n",
      "Epoch 444/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.5202e-04 - val_loss: 0.0593\n",
      "Epoch 445/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.3569e-04 - val_loss: 0.0594\n",
      "Epoch 446/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 3.3328e-04 - val_loss: 0.0596\n",
      "Epoch 447/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 3.3072e-04 - val_loss: 0.0598\n",
      "Epoch 448/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.3262e-04 - val_loss: 0.0600\n",
      "Epoch 449/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.4279e-04 - val_loss: 0.0602\n",
      "Epoch 450/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.4742e-04 - val_loss: 0.0604\n",
      "Epoch 451/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 3.3031e-04 - val_loss: 0.0606\n",
      "Epoch 452/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 3.3418e-04 - val_loss: 0.0608-0\n",
      "Epoch 453/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.2706e-04 - val_loss: 0.0610\n",
      "Epoch 454/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 3.4537e-04 - val_loss: 0.0613\n",
      "Epoch 455/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.3096e-04 - val_loss: 0.0616\n",
      "Epoch 456/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 3.2249e-04 - val_loss: 0.0615\n",
      "Epoch 457/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 3.3540e-04 - val_loss: 0.0618\n",
      "Epoch 458/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 3.3432e-04 - val_loss: 0.0621\n",
      "Epoch 459/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.1828e-04 - val_loss: 0.0624\n",
      "Epoch 460/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 3.1964e-04 - val_loss: 0.0625\n",
      "Epoch 461/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 3.2354e-04 - val_loss: 0.0625\n",
      "Epoch 462/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 3.1419e-04 - val_loss: 0.0627\n",
      "Epoch 463/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 3.1314e-04 - val_loss: 0.0629\n",
      "Epoch 464/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.0911e-04 - val_loss: 0.0632\n",
      "Epoch 465/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 3.0980e-04 - val_loss: 0.0635\n",
      "Epoch 466/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 3.1416e-04 - val_loss: 0.0635\n",
      "Epoch 467/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 3.0838e-04 - val_loss: 0.0639\n",
      "Epoch 468/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 3.1679e-04 - val_loss: 0.0640\n",
      "Epoch 469/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 3.0753e-04 - val_loss: 0.0638\n",
      "Epoch 470/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 3.0105e-04 - val_loss: 0.0644\n",
      "Epoch 471/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.0380e-04 - val_loss: 0.0642\n",
      "Epoch 472/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 3.1985e-04 - val_loss: 0.0646\n",
      "Epoch 473/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 2.9907e-04 - val_loss: 0.0647\n",
      "Epoch 474/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.9700e-04 - val_loss: 0.0650\n",
      "Epoch 475/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 3.1040e-04 - val_loss: 0.0650\n",
      "Epoch 476/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 3.0815e-04 - val_loss: 0.0655\n",
      "Epoch 477/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 3.0148e-04 - val_loss: 0.0655\n",
      "Epoch 478/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 3.0194e-04 - val_loss: 0.0660\n",
      "Epoch 479/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 2.9334e-04 - val_loss: 0.0659\n",
      "Epoch 480/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.9897e-04 - val_loss: 0.0662\n",
      "Epoch 481/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.9270e-04 - val_loss: 0.0664\n",
      "Epoch 482/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 3.0173e-04 - val_loss: 0.0664\n",
      "Epoch 483/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.8624e-04 - val_loss: 0.0667\n",
      "Epoch 484/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.8498e-04 - val_loss: 0.0670\n",
      "Epoch 485/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 2.8051e-04 - val_loss: 0.0673\n",
      "Epoch 486/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 2.8348e-04 - val_loss: 0.0675\n",
      "Epoch 487/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.8262e-04 - val_loss: 0.0675\n",
      "Epoch 488/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 2.7818e-04 - val_loss: 0.0677\n",
      "Epoch 489/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 2.8390e-04 - val_loss: 0.0678\n",
      "Epoch 490/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 2.8908e-04 - val_loss: 0.0682\n",
      "Epoch 491/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.8120e-04 - val_loss: 0.0680\n",
      "Epoch 492/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 2.8004e-04 - val_loss: 0.0683\n",
      "Epoch 493/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.7640e-04 - val_loss: 0.0683\n",
      "Epoch 494/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 2.7495e-04 - val_loss: 0.0684\n",
      "Epoch 495/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 2.7918e-04 - val_loss: 0.0686\n",
      "Epoch 496/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.7662e-04 - val_loss: 0.0688\n",
      "Epoch 497/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.7678e-04 - val_loss: 0.0688\n",
      "Epoch 498/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 2.7242e-04 - val_loss: 0.0691\n",
      "Epoch 499/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 2.7543e-04 - val_loss: 0.0693\n",
      "Epoch 500/1000\n",
      "1168/1168 [==============================] - 0s 237us/step - loss: 2.6680e-04 - val_loss: 0.0697\n",
      "Epoch 501/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 2.7744e-04 - val_loss: 0.0697\n",
      "Epoch 502/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 2.6770e-04 - val_loss: 0.0698\n",
      "Epoch 503/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 2.7065e-04 - val_loss: 0.0699\n",
      "Epoch 504/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 2.6380e-04 - val_loss: 0.0701\n",
      "Epoch 505/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 2.5919e-04 - val_loss: 0.0703\n",
      "Epoch 506/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 2.6003e-04 - val_loss: 0.0705\n",
      "Epoch 507/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 2.6693e-04 - val_loss: 0.0706\n",
      "Epoch 508/1000\n",
      "1168/1168 [==============================] - 0s 218us/step - loss: 2.5735e-04 - val_loss: 0.0707\n",
      "Epoch 509/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 2.6998e-04 - val_loss: 0.0712\n",
      "Epoch 510/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 2.5796e-04 - val_loss: 0.0711\n",
      "Epoch 511/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.5914e-04 - val_loss: 0.0712\n",
      "Epoch 512/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.6278e-04 - val_loss: 0.0714\n",
      "Epoch 513/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.5391e-04 - val_loss: 0.0716\n",
      "Epoch 514/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.5557e-04 - val_loss: 0.0715\n",
      "Epoch 515/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 2.6449e-04 - val_loss: 0.0720\n",
      "Epoch 516/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 2.5513e-04 - val_loss: 0.0722\n",
      "Epoch 517/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.4798e-04 - val_loss: 0.0722\n",
      "Epoch 518/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 2.4115e-04 - val_loss: 0.0724\n",
      "Epoch 519/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 2.5228e-04 - val_loss: 0.0727\n",
      "Epoch 520/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.4760e-04 - val_loss: 0.0728\n",
      "Epoch 521/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.5184e-04 - val_loss: 0.0731\n",
      "Epoch 522/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.4923e-04 - val_loss: 0.0732\n",
      "Epoch 523/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.3966e-04 - val_loss: 0.0733\n",
      "Epoch 524/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 2.3718e-04 - val_loss: 0.0735\n",
      "Epoch 525/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.5473e-04 - val_loss: 0.0736\n",
      "Epoch 526/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.3938e-04 - val_loss: 0.0739\n",
      "Epoch 527/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 2.4232e-04 - val_loss: 0.0740\n",
      "Epoch 528/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.3382e-04 - val_loss: 0.0741\n",
      "Epoch 529/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 2.4726e-04 - val_loss: 0.0743\n",
      "Epoch 530/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.3921e-04 - val_loss: 0.0743\n",
      "Epoch 531/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.4804e-04 - val_loss: 0.0746\n",
      "Epoch 532/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 2.4121e-04 - val_loss: 0.0748\n",
      "Epoch 533/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.4303e-04 - val_loss: 0.0748\n",
      "Epoch 534/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 2.3652e-04 - val_loss: 0.0749\n",
      "Epoch 535/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.3992e-04 - val_loss: 0.0751\n",
      "Epoch 536/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.4023e-04 - val_loss: 0.0753\n",
      "Epoch 537/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 2.3260e-04 - val_loss: 0.0755\n",
      "Epoch 538/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.2494e-04 - val_loss: 0.0755\n",
      "Epoch 539/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.2938e-04 - val_loss: 0.0758\n",
      "Epoch 540/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 2.3428e-04 - val_loss: 0.0761\n",
      "Epoch 541/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.3240e-04 - val_loss: 0.0761\n",
      "Epoch 542/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.2539e-04 - val_loss: 0.0760\n",
      "Epoch 543/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 2.2543e-04 - val_loss: 0.0763\n",
      "Epoch 544/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.2331e-04 - val_loss: 0.0764\n",
      "Epoch 545/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.1763e-04 - val_loss: 0.0766\n",
      "Epoch 546/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 2.3271e-04 - val_loss: 0.0769\n",
      "Epoch 547/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.2194e-04 - val_loss: 0.0773\n",
      "Epoch 548/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 2.2651e-04 - val_loss: 0.0771\n",
      "Epoch 549/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.2340e-04 - val_loss: 0.0772\n",
      "Epoch 550/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 2.2721e-04 - val_loss: 0.0776\n",
      "Epoch 551/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.2059e-04 - val_loss: 0.0775\n",
      "Epoch 552/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.2688e-04 - val_loss: 0.0778\n",
      "Epoch 553/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.2461e-04 - val_loss: 0.0778\n",
      "Epoch 554/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 2.1878e-04 - val_loss: 0.0781\n",
      "Epoch 555/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.2291e-04 - val_loss: 0.0779\n",
      "Epoch 556/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.2340e-04 - val_loss: 0.0785\n",
      "Epoch 557/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 2.2347e-04 - val_loss: 0.0787\n",
      "Epoch 558/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 2.2139e-04 - val_loss: 0.0788-0\n",
      "Epoch 559/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.1313e-04 - val_loss: 0.0786\n",
      "Epoch 560/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 2.2281e-04 - val_loss: 0.0789\n",
      "Epoch 561/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.1779e-04 - val_loss: 0.0792\n",
      "Epoch 562/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 2.1859e-04 - val_loss: 0.0789\n",
      "Epoch 563/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 2.1859e-04 - val_loss: 0.0792\n",
      "Epoch 564/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 2.2013e-04 - val_loss: 0.0796\n",
      "Epoch 565/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.0761e-04 - val_loss: 0.0795\n",
      "Epoch 566/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.1204e-04 - val_loss: 0.0798\n",
      "Epoch 567/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 2.1013e-04 - val_loss: 0.0798\n",
      "Epoch 568/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 2.1025e-04 - val_loss: 0.0802\n",
      "Epoch 569/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 2.0139e-04 - val_loss: 0.0802\n",
      "Epoch 570/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 1.9266e-04 - val_loss: 0.0804\n",
      "Epoch 571/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 2.0401e-04 - val_loss: 0.0808\n",
      "Epoch 572/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.9961e-04 - val_loss: 0.0807\n",
      "Epoch 573/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 2.1134e-04 - val_loss: 0.0807\n",
      "Epoch 574/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.0425e-04 - val_loss: 0.0811\n",
      "Epoch 575/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 2.0268e-04 - val_loss: 0.0811\n",
      "Epoch 576/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.0412e-04 - val_loss: 0.0814\n",
      "Epoch 577/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 2.0764e-04 - val_loss: 0.0814\n",
      "Epoch 578/1000\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 2.0722e-04 - val_loss: 0.0815\n",
      "Epoch 579/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 2.0925e-04 - val_loss: 0.0817\n",
      "Epoch 580/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 1.9214e-04 - val_loss: 0.0817\n",
      "Epoch 581/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 1.9955e-04 - val_loss: 0.0820\n",
      "Epoch 582/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 1.9570e-04 - val_loss: 0.0821\n",
      "Epoch 583/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 1.9254e-04 - val_loss: 0.0822\n",
      "Epoch 584/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 1.8970e-04 - val_loss: 0.0827\n",
      "Epoch 585/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.9967e-04 - val_loss: 0.0825\n",
      "Epoch 586/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.9548e-04 - val_loss: 0.0826\n",
      "Epoch 587/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 2.0430e-04 - val_loss: 0.0829\n",
      "Epoch 588/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 2.0286e-04 - val_loss: 0.0830\n",
      "Epoch 589/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.9714e-04 - val_loss: 0.0831\n",
      "Epoch 590/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.9227e-04 - val_loss: 0.0833\n",
      "Epoch 591/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.8874e-04 - val_loss: 0.0836\n",
      "Epoch 592/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 2.0340e-04 - val_loss: 0.0834\n",
      "Epoch 593/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.9806e-04 - val_loss: 0.0838\n",
      "Epoch 594/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.9183e-04 - val_loss: 0.0838\n",
      "Epoch 595/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.8576e-04 - val_loss: 0.0841\n",
      "Epoch 596/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.8560e-04 - val_loss: 0.0840\n",
      "Epoch 597/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 1.8016e-04 - val_loss: 0.0843\n",
      "Epoch 598/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.8805e-04 - val_loss: 0.0843\n",
      "Epoch 599/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.8537e-04 - val_loss: 0.0843\n",
      "Epoch 600/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.9250e-04 - val_loss: 0.0848\n",
      "Epoch 601/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 1.8014e-04 - val_loss: 0.0849\n",
      "Epoch 602/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.8288e-04 - val_loss: 0.0850\n",
      "Epoch 603/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.8646e-04 - val_loss: 0.0852\n",
      "Epoch 604/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.9151e-04 - val_loss: 0.0851\n",
      "Epoch 605/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.9187e-04 - val_loss: 0.0852\n",
      "Epoch 606/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.9225e-04 - val_loss: 0.0854\n",
      "Epoch 607/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 1.8153e-04 - val_loss: 0.0854\n",
      "Epoch 608/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 1.8011e-04 - val_loss: 0.0858\n",
      "Epoch 609/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.8111e-04 - val_loss: 0.0857\n",
      "Epoch 610/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.7666e-04 - val_loss: 0.0859\n",
      "Epoch 611/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 1.8042e-04 - val_loss: 0.0861\n",
      "Epoch 612/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 1.7466e-04 - val_loss: 0.0858\n",
      "Epoch 613/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 1.8250e-04 - val_loss: 0.0863\n",
      "Epoch 614/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.8297e-04 - val_loss: 0.0862\n",
      "Epoch 615/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.8251e-04 - val_loss: 0.0865\n",
      "Epoch 616/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.7669e-04 - val_loss: 0.0863\n",
      "Epoch 617/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.7222e-04 - val_loss: 0.0867\n",
      "Epoch 618/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 1.8027e-04 - val_loss: 0.0863\n",
      "Epoch 619/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1.7867e-04 - val_loss: 0.0868\n",
      "Epoch 620/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 1.6926e-04 - val_loss: 0.0870\n",
      "Epoch 621/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.6991e-04 - val_loss: 0.0871\n",
      "Epoch 622/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.7319e-04 - val_loss: 0.0873\n",
      "Epoch 623/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.7595e-04 - val_loss: 0.0873\n",
      "Epoch 624/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.8239e-04 - val_loss: 0.0875\n",
      "Epoch 625/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 1.7493e-04 - val_loss: 0.0874\n",
      "Epoch 626/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.7806e-04 - val_loss: 0.0880\n",
      "Epoch 627/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.6964e-04 - val_loss: 0.0879\n",
      "Epoch 628/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.6705e-04 - val_loss: 0.0880\n",
      "Epoch 629/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.6754e-04 - val_loss: 0.0880\n",
      "Epoch 630/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 1.7102e-04 - val_loss: 0.0884\n",
      "Epoch 631/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.7199e-04 - val_loss: 0.0885\n",
      "Epoch 632/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 1.6593e-04 - val_loss: 0.0885\n",
      "Epoch 633/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 1.6211e-04 - val_loss: 0.0889\n",
      "Epoch 634/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.6342e-04 - val_loss: 0.0886\n",
      "Epoch 635/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.6823e-04 - val_loss: 0.0889\n",
      "Epoch 636/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.5858e-04 - val_loss: 0.0888\n",
      "Epoch 637/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.7018e-04 - val_loss: 0.0891\n",
      "Epoch 638/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.7222e-04 - val_loss: 0.0891\n",
      "Epoch 639/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.7057e-04 - val_loss: 0.0892\n",
      "Epoch 640/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.6415e-04 - val_loss: 0.0892\n",
      "Epoch 641/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.6312e-04 - val_loss: 0.0894\n",
      "Epoch 642/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 1.5811e-04 - val_loss: 0.0894\n",
      "Epoch 643/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.7400e-04 - val_loss: 0.0897\n",
      "Epoch 644/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 1.6904e-04 - val_loss: 0.0895\n",
      "Epoch 645/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.5492e-04 - val_loss: 0.0897\n",
      "Epoch 646/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.5325e-04 - val_loss: 0.0898\n",
      "Epoch 647/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.6269e-04 - val_loss: 0.0899\n",
      "Epoch 648/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.7105e-04 - val_loss: 0.0903\n",
      "Epoch 649/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 1.5909e-04 - val_loss: 0.0905\n",
      "Epoch 650/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.6625e-04 - val_loss: 0.0903\n",
      "Epoch 651/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.5530e-04 - val_loss: 0.0906\n",
      "Epoch 652/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1.4978e-04 - val_loss: 0.0906\n",
      "Epoch 653/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 1.5731e-04 - val_loss: 0.0907\n",
      "Epoch 654/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.6008e-04 - val_loss: 0.0906\n",
      "Epoch 655/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.6320e-04 - val_loss: 0.0912\n",
      "Epoch 656/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.6676e-04 - val_loss: 0.0910\n",
      "Epoch 657/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.4944e-04 - val_loss: 0.0912\n",
      "Epoch 658/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.4663e-04 - val_loss: 0.0911\n",
      "Epoch 659/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 1.4943e-04 - val_loss: 0.0914\n",
      "Epoch 660/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 1.5253e-04 - val_loss: 0.0912\n",
      "Epoch 661/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 1.6376e-04 - val_loss: 0.0917\n",
      "Epoch 662/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 1.4356e-04 - val_loss: 0.0916\n",
      "Epoch 663/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 1.4097e-04 - val_loss: 0.0919\n",
      "Epoch 664/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.4605e-0 - 0s 195us/step - loss: 1.4345e-04 - val_loss: 0.0918\n",
      "Epoch 665/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 1.4387e-04 - val_loss: 0.0921\n",
      "Epoch 666/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.4664e-04 - val_loss: 0.0923\n",
      "Epoch 667/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.5129e-04 - val_loss: 0.0925\n",
      "Epoch 668/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.6147e-04 - val_loss: 0.0926\n",
      "Epoch 669/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.5400e-04 - val_loss: 0.0929\n",
      "Epoch 670/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.5075e-04 - val_loss: 0.0928\n",
      "Epoch 671/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.5968e-04 - val_loss: 0.0928\n",
      "Epoch 672/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.5386e-04 - val_loss: 0.0930\n",
      "Epoch 673/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 1.4553e-04 - val_loss: 0.0932\n",
      "Epoch 674/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.4693e-04 - val_loss: 0.0935\n",
      "Epoch 675/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 1.5537e-04 - val_loss: 0.0934\n",
      "Epoch 676/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.4030e-04 - val_loss: 0.0936\n",
      "Epoch 677/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.3522e-04 - val_loss: 0.0937\n",
      "Epoch 678/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.3689e-04 - val_loss: 0.0939\n",
      "Epoch 679/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1.4672e-04 - val_loss: 0.0939\n",
      "Epoch 680/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.3991e-04 - val_loss: 0.0939\n",
      "Epoch 681/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.5445e-04 - val_loss: 0.0943\n",
      "Epoch 682/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.5413e-04 - val_loss: 0.0943\n",
      "Epoch 683/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.3214e-04 - val_loss: 0.0944\n",
      "Epoch 684/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 1.4263e-04 - val_loss: 0.0945\n",
      "Epoch 685/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.4375e-04 - val_loss: 0.0948\n",
      "Epoch 686/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.4703e-04 - val_loss: 0.0949\n",
      "Epoch 687/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.4128e-04 - val_loss: 0.0948\n",
      "Epoch 688/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.3907e-04 - val_loss: 0.0949\n",
      "Epoch 689/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.4212e-04 - val_loss: 0.0950\n",
      "Epoch 690/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.4091e-04 - val_loss: 0.0953\n",
      "Epoch 691/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1.3698e-04 - val_loss: 0.0955\n",
      "Epoch 692/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.4431e-04 - val_loss: 0.0956\n",
      "Epoch 693/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.4661e-04 - val_loss: 0.0957\n",
      "Epoch 694/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.3174e-04 - val_loss: 0.0959\n",
      "Epoch 695/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.3007e-04 - val_loss: 0.0958\n",
      "Epoch 696/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 1.3060e-04 - val_loss: 0.0960\n",
      "Epoch 697/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 1.3783e-04 - val_loss: 0.0965\n",
      "Epoch 698/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.4499e-04 - val_loss: 0.0961\n",
      "Epoch 699/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.4662e-04 - val_loss: 0.0960\n",
      "Epoch 700/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 1.4778e-04 - val_loss: 0.0963\n",
      "Epoch 701/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.4657e-04 - val_loss: 0.0966\n",
      "Epoch 702/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.2997e-04 - val_loss: 0.0966\n",
      "Epoch 703/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.2906e-04 - val_loss: 0.0967\n",
      "Epoch 704/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.2597e-04 - val_loss: 0.0967\n",
      "Epoch 705/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.3363e-04 - val_loss: 0.0969\n",
      "Epoch 706/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.2258e-04 - val_loss: 0.0967\n",
      "Epoch 707/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.3479e-04 - val_loss: 0.0969\n",
      "Epoch 708/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.3947e-04 - val_loss: 0.0973\n",
      "Epoch 709/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 1.4848e-04 - val_loss: 0.0975\n",
      "Epoch 710/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.4455e-04 - val_loss: 0.0975\n",
      "Epoch 711/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.4237e-04 - val_loss: 0.0976\n",
      "Epoch 712/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.3326e-04 - val_loss: 0.0977\n",
      "Epoch 713/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.2191e-04 - val_loss: 0.0976\n",
      "Epoch 714/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.2124e-04 - val_loss: 0.0979\n",
      "Epoch 715/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.3409e-04 - val_loss: 0.0983\n",
      "Epoch 716/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.3553e-04 - val_loss: 0.0982\n",
      "Epoch 717/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.3397e-04 - val_loss: 0.0981\n",
      "Epoch 718/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.3095e-04 - val_loss: 0.0983\n",
      "Epoch 719/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1.2768e-04 - val_loss: 0.0983\n",
      "Epoch 720/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1.3305e-04 - val_loss: 0.0988\n",
      "Epoch 721/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1.3121e-04 - val_loss: 0.0986\n",
      "Epoch 722/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.3572e-04 - val_loss: 0.0987\n",
      "Epoch 723/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.3050e-04 - val_loss: 0.0987\n",
      "Epoch 724/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.2361e-04 - val_loss: 0.0989\n",
      "Epoch 725/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.2949e-04 - val_loss: 0.0989\n",
      "Epoch 726/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 1.2793e-04 - val_loss: 0.0989\n",
      "Epoch 727/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.1711e-04 - val_loss: 0.0989\n",
      "Epoch 728/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.2413e-04 - val_loss: 0.0992\n",
      "Epoch 729/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1.2012e-04 - val_loss: 0.0994\n",
      "Epoch 730/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.3016e-04 - val_loss: 0.0993\n",
      "Epoch 731/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 1.2827e-04 - val_loss: 0.0993\n",
      "Epoch 732/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.4049e-04 - val_loss: 0.0995\n",
      "Epoch 733/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.2122e-04 - val_loss: 0.0997\n",
      "Epoch 734/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 1.1670e-04 - val_loss: 0.1001\n",
      "Epoch 735/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 1.1694e-04 - val_loss: 0.0998\n",
      "Epoch 736/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 1.2006e-04 - val_loss: 0.1003\n",
      "Epoch 737/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 1.1710e-04 - val_loss: 0.1001\n",
      "Epoch 738/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 1.2851e-04 - val_loss: 0.1001\n",
      "Epoch 739/1000\n",
      "1168/1168 [==============================] - 0s 224us/step - loss: 1.2677e-04 - val_loss: 0.1001\n",
      "Epoch 740/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 1.3373e-04 - val_loss: 0.1006\n",
      "Epoch 741/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 1.2797e-04 - val_loss: 0.1004\n",
      "Epoch 742/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 1.1376e-04 - val_loss: 0.1008\n",
      "Epoch 743/1000\n",
      "1168/1168 [==============================] - 0s 227us/step - loss: 1.1194e-04 - val_loss: 0.1005\n",
      "Epoch 744/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 1.1761e-04 - val_loss: 0.1008\n",
      "Epoch 745/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 1.1301e-04 - val_loss: 0.1007\n",
      "Epoch 746/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 1.1762e-04 - val_loss: 0.1011\n",
      "Epoch 747/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 1.2350e-04 - val_loss: 0.1011\n",
      "Epoch 748/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 1.1828e-04 - val_loss: 0.1014\n",
      "Epoch 749/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 1.3556e-04 - val_loss: 0.1008\n",
      "Epoch 750/1000\n",
      "1168/1168 [==============================] - 0s 268us/step - loss: 1.2960e-04 - val_loss: 0.1013\n",
      "Epoch 751/1000\n",
      "1168/1168 [==============================] - 0s 234us/step - loss: 1.1934e-04 - val_loss: 0.1014\n",
      "Epoch 752/1000\n",
      "1168/1168 [==============================] - 0s 229us/step - loss: 1.1422e-04 - val_loss: 0.1014\n",
      "Epoch 753/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 1.2224e-04 - val_loss: 0.1014\n",
      "Epoch 754/1000\n",
      "1168/1168 [==============================] - 0s 257us/step - loss: 1.2149e-04 - val_loss: 0.1014\n",
      "Epoch 755/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 1.1007e-04 - val_loss: 0.1017\n",
      "Epoch 756/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 1.1004e-04 - val_loss: 0.1019\n",
      "Epoch 757/1000\n",
      "1168/1168 [==============================] - 0s 186us/step - loss: 1.1465e-04 - val_loss: 0.1022\n",
      "Epoch 758/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.1872e-04 - val_loss: 0.1021\n",
      "Epoch 759/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 1.1644e-04 - val_loss: 0.1024\n",
      "Epoch 760/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.1699e-04 - val_loss: 0.1022\n",
      "Epoch 761/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.1189e-04 - val_loss: 0.1024\n",
      "Epoch 762/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.0767e-04 - val_loss: 0.1026\n",
      "Epoch 763/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.2038e-04 - val_loss: 0.1028\n",
      "Epoch 764/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.2388e-04 - val_loss: 0.1028\n",
      "Epoch 765/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.1478e-04 - val_loss: 0.1028\n",
      "Epoch 766/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.0812e-04 - val_loss: 0.1032\n",
      "Epoch 767/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.0357e-04 - val_loss: 0.1030\n",
      "Epoch 768/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.0847e-04 - val_loss: 0.1031\n",
      "Epoch 769/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 1.0820e-04 - val_loss: 0.1031\n",
      "Epoch 770/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.1557e-04 - val_loss: 0.1034\n",
      "Epoch 771/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.0860e-04 - val_loss: 0.1032\n",
      "Epoch 772/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 1.0965e-04 - val_loss: 0.1037\n",
      "Epoch 773/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 1.1334e-04 - val_loss: 0.1036\n",
      "Epoch 774/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.2076e-04 - val_loss: 0.1038\n",
      "Epoch 775/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.1028e-04 - val_loss: 0.1040\n",
      "Epoch 776/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.0155e-04 - val_loss: 0.1043\n",
      "Epoch 777/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 1.0026e-04 - val_loss: 0.1038\n",
      "Epoch 778/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.1661e-04 - val_loss: 0.1043\n",
      "Epoch 779/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.1574e-04 - val_loss: 0.1042\n",
      "Epoch 780/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.1080e-04 - val_loss: 0.1044\n",
      "Epoch 781/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1.0361e-04 - val_loss: 0.1045\n",
      "Epoch 782/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.0794e-04 - val_loss: 0.1047\n",
      "Epoch 783/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.1551e-04 - val_loss: 0.1046\n",
      "Epoch 784/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.0256e-04 - val_loss: 0.1050\n",
      "Epoch 785/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 1.0018e-04 - val_loss: 0.1050\n",
      "Epoch 786/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 1.1526e-04 - val_loss: 0.1048\n",
      "Epoch 787/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 1.1538e-04 - val_loss: 0.1049\n",
      "Epoch 788/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.0631e-04 - val_loss: 0.1050\n",
      "Epoch 789/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.0558e-04 - val_loss: 0.1053\n",
      "Epoch 790/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.0093e-04 - val_loss: 0.1055\n",
      "Epoch 791/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 9.9135e-05 - val_loss: 0.1053\n",
      "Epoch 792/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 1.0147e-04 - val_loss: 0.1058\n",
      "Epoch 793/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.0209e-04 - val_loss: 0.1055\n",
      "Epoch 794/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.0834e-04 - val_loss: 0.1061\n",
      "Epoch 795/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.1014e-04 - val_loss: 0.1056\n",
      "Epoch 796/1000\n",
      "1168/1168 [==============================] - 0s 189us/step - loss: 1.1297e-04 - val_loss: 0.1059\n",
      "Epoch 797/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.6566e-05 - val_loss: 0.1061\n",
      "Epoch 798/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.0446e-04 - val_loss: 0.1058\n",
      "Epoch 799/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 1.0739e-04 - val_loss: 0.1058\n",
      "Epoch 800/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.1338e-04 - val_loss: 0.1062\n",
      "Epoch 801/1000\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 1.0532e-04 - val_loss: 0.1063\n",
      "Epoch 802/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 1.0900e-04 - val_loss: 0.1064\n",
      "Epoch 803/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 1.0200e-04 - val_loss: 0.1062\n",
      "Epoch 804/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 1.0144e-04 - val_loss: 0.1065\n",
      "Epoch 805/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 9.6708e-05 - val_loss: 0.1067\n",
      "Epoch 806/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 1.0577e-04 - val_loss: 0.1069\n",
      "Epoch 807/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 1.1081e-04 - val_loss: 0.1067\n",
      "Epoch 808/1000\n",
      "1168/1168 [==============================] - 0s 266us/step - loss: 1.0835e-04 - val_loss: 0.1071\n",
      "Epoch 809/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 9.9002e-05 - val_loss: 0.1069\n",
      "Epoch 810/1000\n",
      "1168/1168 [==============================] - 0s 243us/step - loss: 9.4929e-05 - val_loss: 0.1071\n",
      "Epoch 811/1000\n",
      "1168/1168 [==============================] - 0s 236us/step - loss: 1.0966e-04 - val_loss: 0.1071\n",
      "Epoch 812/1000\n",
      "1168/1168 [==============================] - 0s 227us/step - loss: 1.1105e-04 - val_loss: 0.1073\n",
      "Epoch 813/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1.0177e-04 - val_loss: 0.1072\n",
      "Epoch 814/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 1.0903e-04 - val_loss: 0.1072\n",
      "Epoch 815/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 185us/step - loss: 1.0836e-04 - val_loss: 0.1074\n",
      "Epoch 816/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 9.6778e-05 - val_loss: 0.1075\n",
      "Epoch 817/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 9.4981e-05 - val_loss: 0.1075\n",
      "Epoch 818/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 9.3826e-05 - val_loss: 0.1077\n",
      "Epoch 819/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 1.0131e-04 - val_loss: 0.1080\n",
      "Epoch 820/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 9.4812e-05 - val_loss: 0.1079\n",
      "Epoch 821/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 9.6204e-05 - val_loss: 0.1082\n",
      "Epoch 822/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9.8071e-05 - val_loss: 0.1081\n",
      "Epoch 823/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 9.8901e-05 - val_loss: 0.1083\n",
      "Epoch 824/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 1.0025e-04 - val_loss: 0.1080\n",
      "Epoch 825/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1.0484e-04 - val_loss: 0.1083\n",
      "Epoch 826/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.0677e-04 - val_loss: 0.1082\n",
      "Epoch 827/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.4454e-05 - val_loss: 0.1085\n",
      "Epoch 828/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.6989e-05 - val_loss: 0.1085\n",
      "Epoch 829/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 9.5625e-05 - val_loss: 0.1090-\n",
      "Epoch 830/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.0455e-04 - val_loss: 0.1086\n",
      "Epoch 831/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 9.9639e-05 - val_loss: 0.1090\n",
      "Epoch 832/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.7643e-05 - val_loss: 0.1088\n",
      "Epoch 833/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 9.0944e-05 - val_loss: 0.1089\n",
      "Epoch 834/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1.0040e-04 - val_loss: 0.1091\n",
      "Epoch 835/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.6720e-05 - val_loss: 0.1090\n",
      "Epoch 836/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1.0166e-04 - val_loss: 0.1091\n",
      "Epoch 837/1000\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 9.0482e-05 - val_loss: 0.1092\n",
      "Epoch 838/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 9.6934e-05 - val_loss: 0.1092\n",
      "Epoch 839/1000\n",
      "1168/1168 [==============================] - 0s 193us/step - loss: 1.0748e-04 - val_loss: 0.1091\n",
      "Epoch 840/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1.0345e-04 - val_loss: 0.1095\n",
      "Epoch 841/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9.2482e-05 - val_loss: 0.1094\n",
      "Epoch 842/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 9.9596e-05 - val_loss: 0.1097\n",
      "Epoch 843/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9.9944e-05 - val_loss: 0.1097\n",
      "Epoch 844/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.7193e-05 - val_loss: 0.1100\n",
      "Epoch 845/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.0450e-05 - val_loss: 0.1097\n",
      "Epoch 846/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.7097e-05 - val_loss: 0.1101\n",
      "Epoch 847/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.3520e-05 - val_loss: 0.1099\n",
      "Epoch 848/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 8.9697e-05 - val_loss: 0.1100\n",
      "Epoch 849/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8.4071e-05 - val_loss: 0.1103\n",
      "Epoch 850/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 8.6042e-05 - val_loss: 0.1104\n",
      "Epoch 851/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 8.0700e-05 - val_loss: 0.1103\n",
      "Epoch 852/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.4070e-05 - val_loss: 0.1107\n",
      "Epoch 853/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.4110e-05 - val_loss: 0.1105\n",
      "Epoch 854/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.7114e-05 - val_loss: 0.1107\n",
      "Epoch 855/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 1.0274e-04 - val_loss: 0.1106\n",
      "Epoch 856/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.9750e-05 - val_loss: 0.1110\n",
      "Epoch 857/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 9.3100e-05 - val_loss: 0.1110\n",
      "Epoch 858/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.2337e-05 - val_loss: 0.1113\n",
      "Epoch 859/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.2903e-05 - val_loss: 0.1111\n",
      "Epoch 860/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 9.3633e-05 - val_loss: 0.1114\n",
      "Epoch 861/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 9.4807e-05 - val_loss: 0.1111\n",
      "Epoch 862/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8.3977e-05 - val_loss: 0.1114\n",
      "Epoch 863/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 8.0535e-05 - val_loss: 0.1112\n",
      "Epoch 864/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 8.1849e-05 - val_loss: 0.1116\n",
      "Epoch 865/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 8.6015e-05 - val_loss: 0.1113\n",
      "Epoch 866/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.0950e-05 - val_loss: 0.1116\n",
      "Epoch 867/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 9.1301e-05 - val_loss: 0.1116\n",
      "Epoch 868/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 9.0532e-05 - val_loss: 0.1116\n",
      "Epoch 869/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 9.4426e-05 - val_loss: 0.1121\n",
      "Epoch 870/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 9.7920e-05 - val_loss: 0.1120\n",
      "Epoch 871/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.2423e-05 - val_loss: 0.1120\n",
      "Epoch 872/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 8.8733e-05 - val_loss: 0.1119\n",
      "Epoch 873/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 8.3205e-05 - val_loss: 0.1122\n",
      "Epoch 874/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.2336e-05 - val_loss: 0.1125\n",
      "Epoch 875/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 9.1231e-05 - val_loss: 0.1120\n",
      "Epoch 876/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 9.4099e-05 - val_loss: 0.1124\n",
      "Epoch 877/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 8.7227e-05 - val_loss: 0.1122\n",
      "Epoch 878/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 9.5458e-05 - val_loss: 0.1123\n",
      "Epoch 879/1000\n",
      "1168/1168 [==============================] - 0s 222us/step - loss: 9.5732e-05 - val_loss: 0.1123\n",
      "Epoch 880/1000\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 9.1454e-05 - val_loss: 0.1126\n",
      "Epoch 881/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 7.8934e-05 - val_loss: 0.1124\n",
      "Epoch 882/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 7.7759e-05 - val_loss: 0.1126\n",
      "Epoch 883/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 8.1960e-05 - val_loss: 0.1125\n",
      "Epoch 884/1000\n",
      "1168/1168 [==============================] - 0s 225us/step - loss: 9.6164e-05 - val_loss: 0.1130\n",
      "Epoch 885/1000\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 8.8073e-05 - val_loss: 0.1128\n",
      "Epoch 886/1000\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 8.8115e-05 - val_loss: 0.1130\n",
      "Epoch 887/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 9.0312e-05 - val_loss: 0.1130\n",
      "Epoch 888/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 7.7347e-05 - val_loss: 0.1131\n",
      "Epoch 889/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 8.1223e-05 - val_loss: 0.1133\n",
      "Epoch 890/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 8.8591e-05 - val_loss: 0.1131\n",
      "Epoch 891/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.9360e-05 - val_loss: 0.1132\n",
      "Epoch 892/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.8702e-05 - val_loss: 0.1135\n",
      "Epoch 893/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 8.8813e-05 - val_loss: 0.1132\n",
      "Epoch 894/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 8.6961e-05 - val_loss: 0.1137\n",
      "Epoch 895/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 8.6085e-05 - val_loss: 0.1135\n",
      "Epoch 896/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.4880e-05 - val_loss: 0.1135\n",
      "Epoch 897/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 7.7926e-05 - val_loss: 0.1133\n",
      "Epoch 898/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 9.0821e-05 - val_loss: 0.1139\n",
      "Epoch 899/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 9.1956e-05 - val_loss: 0.1134\n",
      "Epoch 900/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 9.6599e-05 - val_loss: 0.1140\n",
      "Epoch 901/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 8.6025e-05 - val_loss: 0.1138\n",
      "Epoch 902/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 8.7287e-05 - val_loss: 0.1141\n",
      "Epoch 903/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.5978e-05 - val_loss: 0.1139\n",
      "Epoch 904/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.5152e-05 - val_loss: 0.1141\n",
      "Epoch 905/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.6084e-05 - val_loss: 0.1143\n",
      "Epoch 906/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.7408e-05 - val_loss: 0.1143\n",
      "Epoch 907/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 7.3190e-05 - val_loss: 0.1142\n",
      "Epoch 908/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7.9297e-05 - val_loss: 0.1141\n",
      "Epoch 909/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.5616e-05 - val_loss: 0.1145\n",
      "Epoch 910/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.8096e-05 - val_loss: 0.1146\n",
      "Epoch 911/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 8.0589e-05 - val_loss: 0.1146\n",
      "Epoch 912/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.7700e-05 - val_loss: 0.1146\n",
      "Epoch 913/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.2255e-05 - val_loss: 0.1149\n",
      "Epoch 914/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.6232e-05 - val_loss: 0.1148\n",
      "Epoch 915/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 7.7114e-05 - val_loss: 0.1148\n",
      "Epoch 916/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 8.1330e-05 - val_loss: 0.1151\n",
      "Epoch 917/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 8.0061e-05 - val_loss: 0.1148\n",
      "Epoch 918/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 8.7891e-05 - val_loss: 0.1154\n",
      "Epoch 919/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 8.3014e-05 - val_loss: 0.1152\n",
      "Epoch 920/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 8.8022e-05 - val_loss: 0.1151\n",
      "Epoch 921/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 7.7466e-05 - val_loss: 0.1153\n",
      "Epoch 922/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7.7869e-05 - val_loss: 0.1153\n",
      "Epoch 923/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7.5825e-05 - val_loss: 0.1154\n",
      "Epoch 924/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 7.5062e-05 - val_loss: 0.1155\n",
      "Epoch 925/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7.2258e-05 - val_loss: 0.1155\n",
      "Epoch 926/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 7.5646e-05 - val_loss: 0.1157\n",
      "Epoch 927/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 8.2121e-05 - val_loss: 0.1157\n",
      "Epoch 928/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 7.7586e-05 - val_loss: 0.1158\n",
      "Epoch 929/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 8.0943e-05 - val_loss: 0.1160\n",
      "Epoch 930/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.5129e-05 - val_loss: 0.1160\n",
      "Epoch 931/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.0814e-05 - val_loss: 0.1160\n",
      "Epoch 932/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.0729e-05 - val_loss: 0.1161\n",
      "Epoch 933/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.7380e-05 - val_loss: 0.1161\n",
      "Epoch 934/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7.9266e-05 - val_loss: 0.1162\n",
      "Epoch 935/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.8862e-05 - val_loss: 0.1160\n",
      "Epoch 936/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 8.3256e-05 - val_loss: 0.1165\n",
      "Epoch 937/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.0948e-05 - val_loss: 0.1165\n",
      "Epoch 938/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 6.7635e-05 - val_loss: 0.1165\n",
      "Epoch 939/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 6.9284e-05 - val_loss: 0.1166\n",
      "Epoch 940/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 7.3545e-05 - val_loss: 0.1166\n",
      "Epoch 941/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 8.0206e-05 - val_loss: 0.1167\n",
      "Epoch 942/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 8.2263e-05 - val_loss: 0.1171\n",
      "Epoch 943/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.5874e-05 - val_loss: 0.1168\n",
      "Epoch 944/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7.3555e-05 - val_loss: 0.1167\n",
      "Epoch 945/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 7.3868e-05 - val_loss: 0.1171\n",
      "Epoch 946/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 7.5868e-05 - val_loss: 0.1170\n",
      "Epoch 947/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 8.1259e-05 - val_loss: 0.1168\n",
      "Epoch 948/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 7.9069e-05 - val_loss: 0.1170\n",
      "Epoch 949/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 7.4075e-05 - val_loss: 0.1173\n",
      "Epoch 950/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 7.0571e-05 - val_loss: 0.1171\n",
      "Epoch 951/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 7.8791e-05 - val_loss: 0.1172\n",
      "Epoch 952/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 7.3588e-05 - val_loss: 0.1174\n",
      "Epoch 953/1000\n",
      "1168/1168 [==============================] - 0s 264us/step - loss: 6.7796e-05 - val_loss: 0.1175\n",
      "Epoch 954/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 7.3442e-05 - val_loss: 0.1175\n",
      "Epoch 955/1000\n",
      "1168/1168 [==============================] - 0s 253us/step - loss: 7.8150e-05 - val_loss: 0.1176\n",
      "Epoch 956/1000\n",
      "1168/1168 [==============================] - 0s 251us/step - loss: 7.5450e-05 - val_loss: 0.1174\n",
      "Epoch 957/1000\n",
      "1168/1168 [==============================] - 0s 234us/step - loss: 7.6073e-05 - val_loss: 0.1180\n",
      "Epoch 958/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 8.3460e-05 - val_loss: 0.1176\n",
      "Epoch 959/1000\n",
      "1168/1168 [==============================] - 0s 236us/step - loss: 7.6660e-05 - val_loss: 0.1178\n",
      "Epoch 960/1000\n",
      "1168/1168 [==============================] - 0s 240us/step - loss: 6.8070e-05 - val_loss: 0.1179\n",
      "Epoch 961/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 188us/step - loss: 7.5012e-05 - val_loss: 0.1182\n",
      "Epoch 962/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 7.7629e-05 - val_loss: 0.1180\n",
      "Epoch 963/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.5399e-05 - val_loss: 0.1178\n",
      "Epoch 964/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.4384e-05 - val_loss: 0.1183\n",
      "Epoch 965/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 7.5700e-05 - val_loss: 0.1179\n",
      "Epoch 966/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 7.7958e-05 - val_loss: 0.1180\n",
      "Epoch 967/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 8.9040e-05 - val_loss: 0.1185\n",
      "Epoch 968/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.8642e-05 - val_loss: 0.1183\n",
      "Epoch 969/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 6.9226e-05 - val_loss: 0.1181\n",
      "Epoch 970/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 7.2204e-05 - val_loss: 0.1181\n",
      "Epoch 971/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.8575e-05 - val_loss: 0.1181\n",
      "Epoch 972/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.8947e-05 - val_loss: 0.1183\n",
      "Epoch 973/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 7.0671e-05 - val_loss: 0.1183\n",
      "Epoch 974/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 6.7693e-05 - val_loss: 0.1185\n",
      "Epoch 975/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.9518e-05 - val_loss: 0.1185\n",
      "Epoch 976/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 7.4309e-05 - val_loss: 0.1184\n",
      "Epoch 977/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.0273e-05 - val_loss: 0.1188\n",
      "Epoch 978/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 8.2274e-05 - val_loss: 0.1186\n",
      "Epoch 979/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 7.7119e-05 - val_loss: 0.1187\n",
      "Epoch 980/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 7.5852e-05 - val_loss: 0.1187\n",
      "Epoch 981/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.1562e-05 - val_loss: 0.1190\n",
      "Epoch 982/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.0776e-05 - val_loss: 0.1188\n",
      "Epoch 983/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.8322e-05 - val_loss: 0.1191\n",
      "Epoch 984/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 7.1466e-05 - val_loss: 0.1188\n",
      "Epoch 985/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 7.7943e-05 - val_loss: 0.1189\n",
      "Epoch 986/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 7.9441e-05 - val_loss: 0.1189\n",
      "Epoch 987/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 7.0194e-05 - val_loss: 0.1190\n",
      "Epoch 988/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6.6529e-05 - val_loss: 0.1193\n",
      "Epoch 989/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 6.3088e-05 - val_loss: 0.1190\n",
      "Epoch 990/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 5.8391e-05 - val_loss: 0.1197\n",
      "Epoch 991/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 6.7713e-05 - val_loss: 0.1193\n",
      "Epoch 992/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 8.2840e-05 - val_loss: 0.1195\n",
      "Epoch 993/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 7.0622e-05 - val_loss: 0.1197\n",
      "Epoch 994/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 6.9438e-05 - val_loss: 0.1197\n",
      "Epoch 995/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 6.4942e-05 - val_loss: 0.1196\n",
      "Epoch 996/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 7.2964e-05 - val_loss: 0.1197\n",
      "Epoch 997/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 6.8825e-05 - val_loss: 0.1194\n",
      "Epoch 998/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 6.5442e-05 - val_loss: 0.1201\n",
      "Epoch 999/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 7.1845e-05 - val_loss: 0.1199\n",
      "Epoch 1000/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 6.6361e-05 - val_loss: 0.1197\n"
     ]
    }
   ],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 176))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 40, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss='mean_squared_logarithmic_error', optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train, y_train, validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106652.61 ],\n",
       "       [174326.05 ],\n",
       "       [197132.83 ],\n",
       "       ...,\n",
       "       [204139.25 ],\n",
       "       [101044.445],\n",
       "       [243235.44 ]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(ann_pred)\n",
    "Id_df=pd.read_csv('sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub=pd.concat([Id_df['Id'],pred],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.columns=['Id','SalePrice']\n",
    "final_sub.to_csv('submission_ann.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>1.066526e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>1.743260e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>1.971328e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>1.980031e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>2.352071e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1466</td>\n",
       "      <td>1.751555e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467</td>\n",
       "      <td>1.681874e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>1.713944e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1469</td>\n",
       "      <td>1.717579e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1470</td>\n",
       "      <td>1.206936e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1471</td>\n",
       "      <td>1.792418e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1472</td>\n",
       "      <td>1.229181e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1473</td>\n",
       "      <td>1.299921e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1474</td>\n",
       "      <td>1.578094e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1475</td>\n",
       "      <td>1.536149e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1476</td>\n",
       "      <td>4.223642e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1477</td>\n",
       "      <td>2.808278e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1478</td>\n",
       "      <td>2.448441e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1479</td>\n",
       "      <td>2.932961e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1480</td>\n",
       "      <td>4.954162e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1481</td>\n",
       "      <td>3.072059e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1482</td>\n",
       "      <td>2.080511e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1483</td>\n",
       "      <td>1.726988e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1484</td>\n",
       "      <td>1.560890e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1485</td>\n",
       "      <td>1.895528e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1486</td>\n",
       "      <td>1.979386e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1487</td>\n",
       "      <td>3.317250e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1488</td>\n",
       "      <td>2.294519e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1489</td>\n",
       "      <td>2.243499e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1490</td>\n",
       "      <td>2.232405e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2890</td>\n",
       "      <td>8.609430e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2891</td>\n",
       "      <td>1.341701e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2892</td>\n",
       "      <td>5.108828e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2893</td>\n",
       "      <td>4.521373e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2894</td>\n",
       "      <td>5.524951e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2895</td>\n",
       "      <td>3.402601e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2896</td>\n",
       "      <td>2.814705e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2897</td>\n",
       "      <td>1.807065e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2898</td>\n",
       "      <td>1.626853e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2899</td>\n",
       "      <td>1.684766e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2900</td>\n",
       "      <td>1.564537e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2901</td>\n",
       "      <td>2.321485e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2902</td>\n",
       "      <td>2.204285e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2903</td>\n",
       "      <td>3.155908e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2904</td>\n",
       "      <td>3.341532e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2905</td>\n",
       "      <td>1.135638e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2906</td>\n",
       "      <td>2.295243e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2907</td>\n",
       "      <td>1.387794e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2908</td>\n",
       "      <td>1.649450e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2909</td>\n",
       "      <td>1.503688e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2910</td>\n",
       "      <td>7.990102e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2911</td>\n",
       "      <td>8.023487e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2912</td>\n",
       "      <td>1.507900e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2913</td>\n",
       "      <td>6.847638e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2914</td>\n",
       "      <td>7.376665e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>7.489507e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>8.110957e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>2.041392e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>1.010444e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>2.432354e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id     SalePrice\n",
       "0     1461  1.066526e+05\n",
       "1     1462  1.743260e+05\n",
       "2     1463  1.971328e+05\n",
       "3     1464  1.980031e+05\n",
       "4     1465  2.352071e+05\n",
       "5     1466  1.751555e+05\n",
       "6     1467  1.681874e+05\n",
       "7     1468  1.713944e+05\n",
       "8     1469  1.717579e+05\n",
       "9     1470  1.206936e+05\n",
       "10    1471  1.792418e+05\n",
       "11    1472  1.229181e+05\n",
       "12    1473  1.299921e+05\n",
       "13    1474  1.578094e+05\n",
       "14    1475  1.536149e+05\n",
       "15    1476  4.223642e+05\n",
       "16    1477  2.808278e+05\n",
       "17    1478  2.448441e+05\n",
       "18    1479  2.932961e+05\n",
       "19    1480  4.954162e+05\n",
       "20    1481  3.072059e+05\n",
       "21    1482  2.080511e+05\n",
       "22    1483  1.726988e+05\n",
       "23    1484  1.560890e+05\n",
       "24    1485  1.895528e+05\n",
       "25    1486  1.979386e+05\n",
       "26    1487  3.317250e+05\n",
       "27    1488  2.294519e+05\n",
       "28    1489  2.243499e+05\n",
       "29    1490  2.232405e+05\n",
       "...    ...           ...\n",
       "1429  2890  8.609430e+04\n",
       "1430  2891  1.341701e+05\n",
       "1431  2892  5.108828e+04\n",
       "1432  2893  4.521373e+04\n",
       "1433  2894  5.524951e+04\n",
       "1434  2895  3.402601e+05\n",
       "1435  2896  2.814705e+05\n",
       "1436  2897  1.807065e+05\n",
       "1437  2898  1.626853e+05\n",
       "1438  2899  1.684766e+05\n",
       "1439  2900  1.564537e+05\n",
       "1440  2901  2.321485e+05\n",
       "1441  2902  2.204285e+05\n",
       "1442  2903  3.155908e+05\n",
       "1443  2904  3.341532e+05\n",
       "1444  2905  1.135638e+06\n",
       "1445  2906  2.295243e+05\n",
       "1446  2907  1.387794e+05\n",
       "1447  2908  1.649450e+05\n",
       "1448  2909  1.503688e+05\n",
       "1449  2910  7.990102e+04\n",
       "1450  2911  8.023487e+04\n",
       "1451  2912  1.507900e+05\n",
       "1452  2913  6.847638e+04\n",
       "1453  2914  7.376665e+04\n",
       "1454  2915  7.489507e+04\n",
       "1455  2916  8.110957e+04\n",
       "1456  2917  2.041392e+05\n",
       "1457  2918  1.010444e+05\n",
       "1458  2919  2.432354e+05\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <keras.engine.sequential.Sequential object at 0x000002E6183EEEB8> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-797e65d616fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \"\"\"\n\u001b[0;32m    393\u001b[0m     \u001b[1;31m# To ensure multimetric format is not supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                 \u001b[1;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 % estimator)\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[1;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <keras.engine.sequential.Sequential object at 0x000002E6183EEEB8> does not."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(y_pred)\n",
    "Id_df=pd.read_csv('sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub=pd.concat([Id_df['Id'],pred],axis=1)\n",
    "final_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.columns=['Id','SalePrice']\n",
    "final_sub.to_csv('submission_PCA_105.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
